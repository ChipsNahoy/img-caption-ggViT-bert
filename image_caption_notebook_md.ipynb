{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latar Belakang\n",
    "Mengelola dan memahami konten visual dalam jumlah besar secara efisien, terutama untuk meningkatkan aksesibilitas bagi penyandang disabilitas visual dan mendukung pengelolaan arsip digital secara otomatis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from transformers import VisionEncoderDecoderModel, AutoTokenizer, AutoFeatureExtractor, BertTokenizer\n",
    "from PIL import Image\n",
    "import os\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from nltk.translate.bleu_score import corpus_bleu\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Load Dataset (Flickr8k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_path = \"\"\n",
    "images_path = os.path.join(dataset_path, \"Images\")\n",
    "captions_path = os.path.join(dataset_path, \"captions.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Preprocess Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Kelas Flickr8kDataset untuk memproses dataset gambar dan caption\n",
    "class Flickr8kDataset(Dataset):\n",
    "    # Inisialisasi kelas dengan parameter utama\n",
    "    def __init__(self, captions_dict, images_path, feature_extractor, tokenizer, max_len=128):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - captions_dict: Dictionary {image_name: [caption1, caption2, ...]}\n",
    "        - images_path: Path ke direktori gambar\n",
    "        - feature_extractor: Model pretrained untuk memproses gambar\n",
    "        - tokenizer: Tokenizer untuk memproses teks caption\n",
    "        - max_len: Panjang maksimal token caption\n",
    "        \"\"\"\n",
    "        # Konversi dictionary captions_dict menjadi list pasangan (nama_gambar, caption)\n",
    "        self.captions = list(captions_dict.items())\n",
    "        # Simpan direktori gambar\n",
    "        self.images_path = images_path\n",
    "        # Simpan feature extractor\n",
    "        self.feature_extractor = feature_extractor\n",
    "        # Simpan tokenizer\n",
    "        self.tokenizer = tokenizer\n",
    "        # Simpan panjang maksimal token\n",
    "        self.max_len = max_len\n",
    "\n",
    "    # Fungsi untuk menghitung total jumlah data\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Returns:\n",
    "        - Jumlah data dalam dataset\n",
    "        \"\"\"\n",
    "        return len(self.captions)\n",
    "\n",
    "    # Fungsi untuk mengambil data berdasarkan indeks\n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"\n",
    "        Parameters:\n",
    "        - idx: Indeks data yang ingin diambil\n",
    "        \n",
    "        Returns:\n",
    "        - pixel_values: Tensor gambar yang sudah diproses\n",
    "        - input_ids: Token ID dari teks caption\n",
    "        - attention_mask: Mask validasi untuk token caption\n",
    "        \"\"\"\n",
    "        # Ambil nama gambar dan daftar caption berdasarkan indeks\n",
    "        img_name, captions = self.captions[idx]\n",
    "        # Buat path lengkap ke gambar\n",
    "        img_path = os.path.join(self.images_path, img_name)\n",
    "\n",
    "        # Buka gambar dan konversi ke RGB\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        # Ekstrak fitur gambar menggunakan feature extractor\n",
    "        pixel_values = self.feature_extractor(images=image, return_tensors=\"pt\").pixel_values.squeeze()\n",
    "\n",
    "        # Pilih satu caption secara acak untuk mengurangi beban memori\n",
    "        caption = np.random.choice(captions)\n",
    "        # Tokenisasi caption dengan tokenizer\n",
    "        tokenized_caption = self.tokenizer(\n",
    "            caption,\n",
    "            padding=\"max_length\",    # Tambahkan padding untuk mencapai panjang max_len\n",
    "            truncation=True,         # Potong caption jika terlalu panjang\n",
    "            max_length=self.max_len, # Panjang maksimal caption\n",
    "            return_tensors=\"pt\",     # Kembalikan tensor PyTorch\n",
    "        )\n",
    "        # Ambil token ID dari caption\n",
    "        input_ids = tokenized_caption.input_ids.squeeze()\n",
    "        # Ambil attention mask dari caption\n",
    "        attention_mask = tokenized_caption.attention_mask.squeeze()\n",
    "\n",
    "        # Kembalikan tensor gambar, token ID, dan attention mask\n",
    "        return pixel_values, input_ids, attention_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load captions dari file teks ke dalam dictionary\n",
    "captions_dict = {}\n",
    "\n",
    "# Buka file captions.txt untuk membaca data\n",
    "with open(captions_path, 'r') as file:\n",
    "    # Lewati baris pertama (header) dan baca baris selanjutnya satu per satu\n",
    "    for line in file.readlines()[1:]:\n",
    "        # Pisahkan baris menjadi nama gambar dan caption berdasarkan koma pertama\n",
    "        img_name, caption = line.strip().split(\",\", 1)\n",
    "        # Tambahkan token \"startseq\" di awal dan \"endseq\" di akhir setiap caption\n",
    "        caption = \"startseq \" + caption.strip() + \" endseq\"\n",
    "        \n",
    "        # Jika nama gambar belum ada di dictionary, inisialisasi daftar untuk caption\n",
    "        if img_name not in captions_dict:\n",
    "            captions_dict[img_name] = []\n",
    "        # Tambahkan caption ke daftar caption untuk gambar tersebut\n",
    "        captions_dict[img_name].append(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\School Things\\Uni\\IlmuData2\\deep-learning\\Lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer and feature extractor\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Define Model (Vision Transformer + Text Decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertLMHeadModel were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['bert.encoder.layer.0.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.0.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.0.crossattention.output.dense.bias', 'bert.encoder.layer.0.crossattention.output.dense.weight', 'bert.encoder.layer.0.crossattention.self.key.bias', 'bert.encoder.layer.0.crossattention.self.key.weight', 'bert.encoder.layer.0.crossattention.self.query.bias', 'bert.encoder.layer.0.crossattention.self.query.weight', 'bert.encoder.layer.0.crossattention.self.value.bias', 'bert.encoder.layer.0.crossattention.self.value.weight', 'bert.encoder.layer.1.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.1.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.1.crossattention.output.dense.bias', 'bert.encoder.layer.1.crossattention.output.dense.weight', 'bert.encoder.layer.1.crossattention.self.key.bias', 'bert.encoder.layer.1.crossattention.self.key.weight', 'bert.encoder.layer.1.crossattention.self.query.bias', 'bert.encoder.layer.1.crossattention.self.query.weight', 'bert.encoder.layer.1.crossattention.self.value.bias', 'bert.encoder.layer.1.crossattention.self.value.weight', 'bert.encoder.layer.10.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.10.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.10.crossattention.output.dense.bias', 'bert.encoder.layer.10.crossattention.output.dense.weight', 'bert.encoder.layer.10.crossattention.self.key.bias', 'bert.encoder.layer.10.crossattention.self.key.weight', 'bert.encoder.layer.10.crossattention.self.query.bias', 'bert.encoder.layer.10.crossattention.self.query.weight', 'bert.encoder.layer.10.crossattention.self.value.bias', 'bert.encoder.layer.10.crossattention.self.value.weight', 'bert.encoder.layer.11.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.11.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.11.crossattention.output.dense.bias', 'bert.encoder.layer.11.crossattention.output.dense.weight', 'bert.encoder.layer.11.crossattention.self.key.bias', 'bert.encoder.layer.11.crossattention.self.key.weight', 'bert.encoder.layer.11.crossattention.self.query.bias', 'bert.encoder.layer.11.crossattention.self.query.weight', 'bert.encoder.layer.11.crossattention.self.value.bias', 'bert.encoder.layer.11.crossattention.self.value.weight', 'bert.encoder.layer.2.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.2.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.2.crossattention.output.dense.bias', 'bert.encoder.layer.2.crossattention.output.dense.weight', 'bert.encoder.layer.2.crossattention.self.key.bias', 'bert.encoder.layer.2.crossattention.self.key.weight', 'bert.encoder.layer.2.crossattention.self.query.bias', 'bert.encoder.layer.2.crossattention.self.query.weight', 'bert.encoder.layer.2.crossattention.self.value.bias', 'bert.encoder.layer.2.crossattention.self.value.weight', 'bert.encoder.layer.3.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.3.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.3.crossattention.output.dense.bias', 'bert.encoder.layer.3.crossattention.output.dense.weight', 'bert.encoder.layer.3.crossattention.self.key.bias', 'bert.encoder.layer.3.crossattention.self.key.weight', 'bert.encoder.layer.3.crossattention.self.query.bias', 'bert.encoder.layer.3.crossattention.self.query.weight', 'bert.encoder.layer.3.crossattention.self.value.bias', 'bert.encoder.layer.3.crossattention.self.value.weight', 'bert.encoder.layer.4.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.4.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.4.crossattention.output.dense.bias', 'bert.encoder.layer.4.crossattention.output.dense.weight', 'bert.encoder.layer.4.crossattention.self.key.bias', 'bert.encoder.layer.4.crossattention.self.key.weight', 'bert.encoder.layer.4.crossattention.self.query.bias', 'bert.encoder.layer.4.crossattention.self.query.weight', 'bert.encoder.layer.4.crossattention.self.value.bias', 'bert.encoder.layer.4.crossattention.self.value.weight', 'bert.encoder.layer.5.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.5.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.5.crossattention.output.dense.bias', 'bert.encoder.layer.5.crossattention.output.dense.weight', 'bert.encoder.layer.5.crossattention.self.key.bias', 'bert.encoder.layer.5.crossattention.self.key.weight', 'bert.encoder.layer.5.crossattention.self.query.bias', 'bert.encoder.layer.5.crossattention.self.query.weight', 'bert.encoder.layer.5.crossattention.self.value.bias', 'bert.encoder.layer.5.crossattention.self.value.weight', 'bert.encoder.layer.6.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.6.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.6.crossattention.output.dense.bias', 'bert.encoder.layer.6.crossattention.output.dense.weight', 'bert.encoder.layer.6.crossattention.self.key.bias', 'bert.encoder.layer.6.crossattention.self.key.weight', 'bert.encoder.layer.6.crossattention.self.query.bias', 'bert.encoder.layer.6.crossattention.self.query.weight', 'bert.encoder.layer.6.crossattention.self.value.bias', 'bert.encoder.layer.6.crossattention.self.value.weight', 'bert.encoder.layer.7.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.7.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.7.crossattention.output.dense.bias', 'bert.encoder.layer.7.crossattention.output.dense.weight', 'bert.encoder.layer.7.crossattention.self.key.bias', 'bert.encoder.layer.7.crossattention.self.key.weight', 'bert.encoder.layer.7.crossattention.self.query.bias', 'bert.encoder.layer.7.crossattention.self.query.weight', 'bert.encoder.layer.7.crossattention.self.value.bias', 'bert.encoder.layer.7.crossattention.self.value.weight', 'bert.encoder.layer.8.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.8.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.8.crossattention.output.dense.bias', 'bert.encoder.layer.8.crossattention.output.dense.weight', 'bert.encoder.layer.8.crossattention.self.key.bias', 'bert.encoder.layer.8.crossattention.self.key.weight', 'bert.encoder.layer.8.crossattention.self.query.bias', 'bert.encoder.layer.8.crossattention.self.query.weight', 'bert.encoder.layer.8.crossattention.self.value.bias', 'bert.encoder.layer.8.crossattention.self.value.weight', 'bert.encoder.layer.9.crossattention.output.LayerNorm.bias', 'bert.encoder.layer.9.crossattention.output.LayerNorm.weight', 'bert.encoder.layer.9.crossattention.output.dense.bias', 'bert.encoder.layer.9.crossattention.output.dense.weight', 'bert.encoder.layer.9.crossattention.self.key.bias', 'bert.encoder.layer.9.crossattention.self.key.weight', 'bert.encoder.layer.9.crossattention.self.query.bias', 'bert.encoder.layer.9.crossattention.self.query.weight', 'bert.encoder.layer.9.crossattention.self.value.bias', 'bert.encoder.layer.9.crossattention.self.value.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# Inisialisasi Vision Encoder Decoder Model\n",
    "model = VisionEncoderDecoderModel.from_encoder_decoder_pretrained(\n",
    "    \"google/vit-base-patch16-224-in21k\",  # Model Vision Transformer (ViT) sebagai encoder\n",
    "    \"bert-base-uncased\"                  # Model BERT sebagai decoder\n",
    ")\n",
    "\n",
    "# Konfigurasi parameter model\n",
    "# ID token awal untuk decoder, diambil dari tokenizer\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "# ID token padding untuk mengisi token kosong\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "# Ukuran vocab decoder, diambil dari konfigurasi decoder\n",
    "model.config.vocab_size = model.config.decoder.vocab_size\n",
    "# ID token akhir, diambil dari tokenizer\n",
    "model.config.eos_token_id = tokenizer.sep_token_id\n",
    "# Panjang maksimal caption yang akan dihasilkan\n",
    "model.config.max_length = 128\n",
    "\n",
    "# Set parameter pelatihan tambahan\n",
    "# Token awal bos (Beginning of Sequence), digunakan untuk memulai decoding\n",
    "model.config.bos_token_id = tokenizer.cls_token_id\n",
    "# Memaksa token awal bos untuk setiap decoding\n",
    "model.config.forced_bos_token_id = tokenizer.cls_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "VisionEncoderDecoderModel(\n",
       "  (encoder): ViTModel(\n",
       "    (embeddings): ViTEmbeddings(\n",
       "      (patch_embeddings): ViTPatchEmbeddings(\n",
       "        (projection): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16))\n",
       "      )\n",
       "      (dropout): Dropout(p=0.0, inplace=False)\n",
       "    )\n",
       "    (encoder): ViTEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0-11): 12 x ViTLayer(\n",
       "          (attention): ViTSdpaAttention(\n",
       "            (attention): ViTSdpaSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "            (output): ViTSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.0, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): ViTIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "            (intermediate_act_fn): GELUActivation()\n",
       "          )\n",
       "          (output): ViTOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.0, inplace=False)\n",
       "          )\n",
       "          (layernorm_before): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (layernorm_after): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (layernorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (pooler): ViTPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (decoder): BertLMHeadModel(\n",
       "    (bert): BertModel(\n",
       "      (embeddings): BertEmbeddings(\n",
       "        (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "        (position_embeddings): Embedding(512, 768)\n",
       "        (token_type_embeddings): Embedding(2, 768)\n",
       "        (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        (dropout): Dropout(p=0.1, inplace=False)\n",
       "      )\n",
       "      (encoder): BertEncoder(\n",
       "        (layer): ModuleList(\n",
       "          (0-11): 12 x BertLayer(\n",
       "            (attention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (crossattention): BertAttention(\n",
       "              (self): BertSdpaSelfAttention(\n",
       "                (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "              (output): BertSelfOutput(\n",
       "                (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "                (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "                (dropout): Dropout(p=0.1, inplace=False)\n",
       "              )\n",
       "            )\n",
       "            (intermediate): BertIntermediate(\n",
       "              (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "              (intermediate_act_fn): GELUActivation()\n",
       "            )\n",
       "            (output): BertOutput(\n",
       "              (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (cls): BertOnlyMLMHead(\n",
       "      (predictions): BertLMPredictionHead(\n",
       "        (transform): BertPredictionHeadTransform(\n",
       "          (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "          (transform_act_fn): GELUActivation()\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "        )\n",
       "        (decoder): Linear(in_features=768, out_features=30522, bias=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Inisialisasi optimizer AdamW\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-5)\n",
    "\"\"\"\n",
    "Penjelasan:\n",
    "- Optimizer AdamW digunakan untuk mengatur pembaruan parameter model selama pelatihan.\n",
    "- AdamW adalah varian dari algoritma Adam dengan regularisasi weight decay untuk mengurangi overfitting.\n",
    "- `model.parameters()`: Parameter model yang akan diperbarui.\n",
    "- `lr=5e-5`: Learning rate, menentukan seberapa besar langkah pembaruan parameter.\n",
    "\"\"\"\n",
    "\n",
    "# Menentukan perangkat untuk pelatihan (GPU atau CPU)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\"\"\"\n",
    "Penjelasan:\n",
    "- `torch.device`: Menentukan perangkat yang akan digunakan untuk pelatihan.\n",
    "- `torch.cuda.is_available()`: Mengecek apakah GPU tersedia di sistem.\n",
    "- Jika GPU tersedia, pelatihan akan dilakukan di GPU. Jika tidak, akan dilakukan di CPU.\n",
    "\"\"\"\n",
    "\n",
    "# Memindahkan model ke perangkat yang ditentukan\n",
    "model.to(device)\n",
    "\"\"\"\n",
    "Penjelasan:\n",
    "- Memindahkan seluruh parameter model ke perangkat (GPU atau CPU) agar dapat diproses sesuai dengan perangkat yang tersedia.\n",
    "- Jika menggunakan GPU, ini memungkinkan model memanfaatkan akselerasi hardware untuk pelatihan.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data Loading**\n",
    "\n",
    "> `batch_size` of **4** will use ~5gb of memory\n",
    "\n",
    "> `batch_size` of **8** will use ~5.6gb of memory\n",
    "\n",
    "> `batch_size` of **16** will use ~8gb of memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Membagi data menjadi train dan test set\n",
    "train_captions, test_captions = train_test_split(\n",
    "    list(captions_dict.items()),  # Mengubah captions_dict menjadi list pasangan (img_name, captions)\n",
    "    test_size=0.2,               # Menggunakan 20% data untuk test set\n",
    "    random_state=69              # Seed random untuk memastikan hasil pembagian yang konsisten\n",
    ")\n",
    "\"\"\"\n",
    "Penjelasan:\n",
    "- `train_test_split`: Fungsi untuk membagi data menjadi train set dan test set.\n",
    "- `list(captions_dict.items())`: Mengubah dictionary captions_dict menjadi list pasangan (image_name, captions).\n",
    "- `test_size=0.2`: 20% data digunakan sebagai test set, sisanya untuk train set.\n",
    "- `random_state=69`: Menentukan seed untuk membagi data secara acak dengan hasil yang konsisten.\n",
    "\"\"\"\n",
    "\n",
    "# Membuat objek dataset untuk training dan testing\n",
    "train_dataset = Flickr8kDataset(dict(train_captions), images_path, feature_extractor, tokenizer)\n",
    "test_dataset = Flickr8kDataset(dict(test_captions), images_path, feature_extractor, tokenizer)\n",
    "\"\"\"\n",
    "Penjelasan:\n",
    "- `Flickr8kDataset`: Kelas dataset yang memproses gambar dan caption.\n",
    "- `dict(train_captions)`: Mengubah list pasangan (img_name, captions) kembali menjadi dictionary untuk training.\n",
    "- `dict(test_captions)`: Mengubah list pasangan (img_name, captions) kembali menjadi dictionary untuk testing.\n",
    "- Parameter lain:\n",
    "  - `images_path`: Path ke direktori gambar.\n",
    "  - `feature_extractor`: Model pretrained untuk memproses gambar.\n",
    "  - `tokenizer`: Tokenizer untuk memproses teks caption.\n",
    "\"\"\"\n",
    "\n",
    "# Membuat DataLoader untuk training dan testing\n",
    "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8)\n",
    "\"\"\"\n",
    "Penjelasan:\n",
    "- `DataLoader`: Membungkus dataset untuk mempermudah iterasi selama pelatihan.\n",
    "- Parameter:\n",
    "  - `train_dataset` dan `test_dataset`: Dataset untuk training dan testing.\n",
    "  - `batch_size=8`: Jumlah data yang diproses sekaligus dalam satu batch.\n",
    "  - `shuffle=True` (hanya untuk training): Mengacak data untuk memastikan pelatihan lebih bervariasi.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5, Loss: 0.29994874144587735\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/5, Loss: 0.27464860090145515\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/5, Loss: 0.25878962841257913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/5, Loss: 0.24435266159815605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                              "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/5, Loss: 0.23278936003283016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "# Jumlah epoch untuk pelatihan\n",
    "epochs = 5\n",
    "\"\"\"\n",
    "Penjelasan:\n",
    "- `epochs`: Jumlah iterasi penuh melalui seluruh dataset selama pelatihan.\n",
    "- Dalam hal ini, model akan dilatih selama 5 epoch.\n",
    "\"\"\"\n",
    "\n",
    "# Loop utama untuk setiap epoch\n",
    "for epoch in range(epochs):\n",
    "    # Mengatur model dalam mode pelatihan\n",
    "    model.train()\n",
    "    # Inisialisasi total loss untuk epoch ini\n",
    "    total_loss = 0\n",
    "    # Membuat iterator dengan progres bar menggunakan tqdm\n",
    "    batch_iterator = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\", leave=False)\n",
    "\n",
    "    # Loop untuk setiap batch dalam DataLoader\n",
    "    for pixel_values, input_ids, attention_mask in batch_iterator:\n",
    "        # Memindahkan data ke perangkat (CPU/GPU)\n",
    "        pixel_values = pixel_values.to(device)\n",
    "        input_ids = input_ids.to(device)\n",
    "        attention_mask = attention_mask.to(device)\n",
    "\n",
    "        # Forward pass: Menghitung output dan loss\n",
    "        outputs = model(\n",
    "            pixel_values=pixel_values,        # Tensor gambar yang sudah diproses\n",
    "            labels=input_ids,                 # Caption sebagai label\n",
    "            decoder_attention_mask=attention_mask  # Attention mask untuk decoder\n",
    "        )\n",
    "        # Loss dari output model\n",
    "        loss = outputs.loss\n",
    "        # Tambahkan loss batch ke total_loss\n",
    "        total_loss += loss.item()\n",
    "\n",
    "        # Backward pass: Menghitung gradien\n",
    "        loss.backward()\n",
    "        # Update parameter model dengan optimizer\n",
    "        optimizer.step()\n",
    "        # Reset gradien untuk batch berikutnya\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Menampilkan nilai loss untuk batch ini di tqdm\n",
    "        batch_iterator.set_postfix(batch_loss=loss.item())\n",
    "\n",
    "    # Menampilkan loss rata-rata untuk epoch ini\n",
    "    print(f\"Epoch {epoch + 1}/{epochs}, Loss: {total_loss / len(train_loader)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\School Things\\Uni\\IlmuData2\\deep-learning\\Lib\\site-packages\\transformers\\modeling_utils.py:2817: UserWarning: Moving the following attributes in the config to the generation config: {'max_length': 128, 'forced_bos_token_id': 101}. You are seeing this warning because you've set generation parameters in the model config, as opposed to in the generation config.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['./img_caption_googlevit_bert_tts\\\\preprocessor_config.json']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Menyimpan model yang sudah dilatih, tokenizer, dan feature extractor\n",
    "model.save_pretrained(\"./img_caption_googlevit_bert_tts\")\n",
    "\"\"\"\n",
    "Penjelasan:\n",
    "- `save_pretrained`: Fungsi untuk menyimpan model beserta konfigurasinya.\n",
    "- `./img_caption_googlevit_bert_tts`: Direktori tujuan untuk menyimpan model.\n",
    "- Hasil penyimpanan meliputi:\n",
    "  - File konfigurasi model (`config.json`).\n",
    "  - Parameter model dalam format PyTorch (`pytorch_model.bin`).\n",
    "\"\"\"\n",
    "\n",
    "tokenizer.save_pretrained(\"./img_caption_googlevit_bert_tts\")\n",
    "\"\"\"\n",
    "Penjelasan:\n",
    "- Menyimpan tokenizer yang digunakan untuk memproses caption.\n",
    "- Hasil penyimpanan meliputi:\n",
    "  - File konfigurasi tokenizer (`tokenizer_config.json`).\n",
    "  - File vocab/tokenizer (`vocab.txt` atau file terkait lainnya).\n",
    "- Menjamin bahwa tokenizer yang sama dapat digunakan saat model diload kembali.\n",
    "\"\"\"\n",
    "\n",
    "feature_extractor.save_pretrained(\"./img_caption_googlevit_bert_tts\")\n",
    "\"\"\"\n",
    "Penjelasan:\n",
    "- Menyimpan feature extractor yang digunakan untuk memproses gambar.\n",
    "- Hasil penyimpanan meliputi:\n",
    "  - File konfigurasi extractor (`preprocessor_config.json` atau serupa).\n",
    "- Memastikan proses preprocessing gambar dapat direproduksi dengan presisi yang sama saat model digunakan kembali.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\School Things\\Uni\\IlmuData2\\deep-learning\\Lib\\site-packages\\transformers\\models\\vit\\feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'\\nPenjelasan:\\n- Memindahkan seluruh parameter model ke perangkat yang sesuai (GPU/CPU).\\n- Memastikan kompatibilitas antara data dan perangkat pemrosesan.\\n'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Memuat kembali model Vision Encoder Decoder\n",
    "model = VisionEncoderDecoderModel.from_pretrained(\"./img_caption_tf_googlevit_bert\")\n",
    "\"\"\"\n",
    "Penjelasan:\n",
    "- `from_pretrained`: Fungsi untuk memuat model yang telah disimpan sebelumnya.\n",
    "- \"./img_caption_tf_googlevit_bert\": Direktori tempat model yang telah dilatih disimpan.\n",
    "- Hasil pemuatan meliputi konfigurasi dan parameter model.\n",
    "\"\"\"\n",
    "\n",
    "# Memuat tokenizer dari direktori yang sama\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"./img_caption_tf_googlevit_bert\")\n",
    "\"\"\"\n",
    "Penjelasan:\n",
    "- `from_pretrained`: Memuat tokenizer yang digunakan saat pelatihan.\n",
    "- Memastikan proses tokenisasi teks sama dengan yang digunakan selama pelatihan.\n",
    "\"\"\"\n",
    "\n",
    "# Memuat feature extractor dari model pretrained\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\"\"\"\n",
    "Penjelasan:\n",
    "- `from_pretrained`: Memuat feature extractor Vision Transformer.\n",
    "- Feature extractor memastikan gambar diproses ke format yang sesuai untuk encoder.\n",
    "\"\"\"\n",
    "\n",
    "# Mengonfigurasi token awal, akhir, padding, dan lainnya untuk decoder\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "\"\"\"\n",
    "Penjelasan:\n",
    "- `decoder_start_token_id`: Token awal yang digunakan oleh decoder untuk memulai prediksi teks.\n",
    "\"\"\"\n",
    "\n",
    "model.config.bos_token_id = tokenizer.cls_token_id\n",
    "\"\"\"\n",
    "Penjelasan:\n",
    "- `bos_token_id`: Token BOS (Beginning of Sequence) menandai awal teks untuk decoding.\n",
    "\"\"\"\n",
    "\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\"\"\"\n",
    "Penjelasan:\n",
    "- `pad_token_id`: Token untuk padding digunakan saat teks lebih pendek dari panjang maksimum.\n",
    "\"\"\"\n",
    "\n",
    "model.config.eos_token_id = tokenizer.sep_token_id\n",
    "\"\"\"\n",
    "Penjelasan:\n",
    "- `eos_token_id`: Token EOS (End of Sequence) menandai akhir teks untuk decoding.\n",
    "\"\"\"\n",
    "\n",
    "model.generation_config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "\"\"\"\n",
    "Penjelasan:\n",
    "- `generation_config`: Konfigurasi khusus untuk proses teks generation.\n",
    "- `decoder_start_token_id`: Token awal untuk setiap caption yang dihasilkan.\n",
    "\"\"\"\n",
    "\n",
    "# Menentukan perangkat (GPU atau CPU) untuk pelatihan atau inferensi\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\"\"\"\n",
    "Penjelasan:\n",
    "- `torch.device`: Menentukan perangkat yang akan digunakan untuk pemrosesan.\n",
    "- Jika GPU tersedia, `cuda` akan digunakan. Jika tidak, CPU akan digunakan.\n",
    "\"\"\"\n",
    "\n",
    "# Memindahkan model ke perangkat yang ditentukan\n",
    "model.to(device)\n",
    "\"\"\"\n",
    "Penjelasan:\n",
    "- Memindahkan seluruh parameter model ke perangkat yang sesuai (GPU/CPU).\n",
    "- Memastikan kompatibilitas antara data dan perangkat pemrosesan.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Konfigurasi model untuk decoding\n",
    "model.config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.config.bos_token_id = tokenizer.cls_token_id\n",
    "model.config.eos_token_id = tokenizer.sep_token_id\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\"\"\"\n",
    "Penjelasan:\n",
    "- `decoder_start_token_id`: Token awal untuk memulai decoding.\n",
    "- `bos_token_id`: Token BOS (Beginning of Sequence) yang menandai awal teks.\n",
    "- `eos_token_id`: Token EOS (End of Sequence) yang menandai akhir teks.\n",
    "- `pad_token_id`: Token padding untuk mengisi token kosong dalam teks pendek.\n",
    "\"\"\"\n",
    "\n",
    "# Konfigurasi tambahan untuk proses generasi\n",
    "model.generation_config.decoder_start_token_id = tokenizer.cls_token_id\n",
    "model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
    "model.generation_config.eos_token_id = tokenizer.sep_token_id\n",
    "\"\"\"\n",
    "Penjelasan:\n",
    "- `generation_config`: Digunakan untuk mengatur parameter generasi teks.\n",
    "- `decoder_start_token_id`: Token awal untuk setiap caption yang dihasilkan.\n",
    "- `pad_token_id`: Token padding untuk memastikan teks seragam.\n",
    "- `eos_token_id`: Token akhir untuk menandai selesainya caption.\n",
    "\"\"\"\n",
    "\n",
    "# Fungsi untuk mengevaluasi model\n",
    "def evaluate_model(model, dataloader, tokenizer, feature_extractor, device):\n",
    "    model.eval()  # Mengatur model ke mode evaluasi\n",
    "    actual, predicted = [], []  # List untuk menyimpan caption asli dan hasil prediksi\n",
    "\n",
    "    with torch.no_grad():  # Tidak menghitung gradien selama evaluasi\n",
    "        for pixel_values, input_ids, attention_mask in tqdm(dataloader, desc=\"Evaluating\", leave=False):\n",
    "            # Pindahkan data ke perangkat (CPU/GPU)\n",
    "            pixel_values = pixel_values.to(device)\n",
    "            captions = input_ids.tolist()  # Konversi tensor ke list ID token\n",
    "\n",
    "            # Generate captions\n",
    "            output_ids = model.generate(pixel_values, max_length=128, num_beams=5)\n",
    "            \"\"\"\n",
    "            Penjelasan:\n",
    "            - `generate`: Fungsi untuk menghasilkan teks dari model.\n",
    "            - `max_length=128`: Panjang maksimal caption yang dihasilkan.\n",
    "            - `num_beams=5`: Menggunakan beam search untuk menghasilkan teks terbaik.\n",
    "            \"\"\"\n",
    "\n",
    "            # Decode hasil prediksi menjadi teks\n",
    "            decoded_predictions = [tokenizer.decode(ids, skip_special_tokens=True) for ids in output_ids]\n",
    "\n",
    "            # Decode caption asli menjadi teks\n",
    "            decoded_actual = [tokenizer.decode(ids, skip_special_tokens=True) for ids in captions]\n",
    "\n",
    "            # Tambahkan hasil ke list actual dan predicted\n",
    "            predicted.extend(decoded_predictions)\n",
    "            actual.extend([[caption] for caption in decoded_actual])  # BLEU membutuhkan list of list\n",
    "\n",
    "    # Menghitung skor BLEU\n",
    "    bleu_score = corpus_bleu(actual, predicted)\n",
    "    \"\"\"\n",
    "    Penjelasan:\n",
    "    - `corpus_bleu`: Fungsi dari nltk untuk menghitung skor BLEU.\n",
    "    - `actual`: Caption asli (list of list).\n",
    "    - `predicted`: Caption yang dihasilkan oleh model.\n",
    "    \"\"\"\n",
    "    print(f\"BLEU Score: {bleu_score:.4f}\")\n",
    "    return bleu_score  # Mengembalikan skor BLEU\n",
    "\n",
    "# Fungsi untuk menghasilkan caption dari gambar\n",
    "def generate_caption(image_path, model, tokenizer, feature_extractor, max_length=128):\n",
    "    # Preprocessing gambar\n",
    "    image = Image.open(image_path).convert(\"RGB\")  # Membuka gambar dan mengonversi ke RGB\n",
    "    pixel_values = feature_extractor(images=image, return_tensors=\"pt\").pixel_values.to(device)\n",
    "    \"\"\"\n",
    "    Penjelasan:\n",
    "    - Gambar diproses menggunakan feature_extractor agar sesuai dengan input encoder.\n",
    "    - `return_tensors=\"pt\"`: Mengembalikan tensor PyTorch.\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate caption dari gambar\n",
    "    output_ids = model.generate(pixel_values, max_length=max_length, num_beams=5)\n",
    "    \"\"\"\n",
    "    Penjelasan:\n",
    "    - `generate`: Menghasilkan teks berdasarkan gambar yang diproses.\n",
    "    - `max_length`: Panjang maksimal teks yang dihasilkan.\n",
    "    - `num_beams`: Beam search untuk menghasilkan teks terbaik.\n",
    "    \"\"\"\n",
    "\n",
    "    # Decode hasil prediksi menjadi teks\n",
    "    caption = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "    # Membersihkan teks dengan menghapus token tambahan\n",
    "    caption = caption.replace(\"startseq\", \"\").replace(\"endseq\", \"\").strip()\n",
    "    return caption  # Mengembalikan teks caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating:   0%|          | 0/203 [00:00<?, ?it/s]We strongly recommend passing in an `attention_mask` since your input_ids may be padded. See https://huggingface.co/docs/transformers/troubleshooting#incorrect-output-when-padding-tokens-arent-masked.\n",
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BLEU Score: 0.4614\n"
     ]
    }
   ],
   "source": [
    "bleu = evaluate_model(model, test_loader, tokenizer, feature_extractor, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inisialisasi list untuk menyimpan sampel\n",
    "samples = []\n",
    "\"\"\"\n",
    "Penjelasan:\n",
    "- `samples`: List untuk menyimpan data evaluasi, termasuk nama gambar, caption asli, dan caption hasil prediksi.\n",
    "\"\"\"\n",
    "\n",
    "# Iterasi melalui beberapa data uji\n",
    "for i in range(5):  # Ubah range untuk menyimpan lebih banyak contoh\n",
    "    img_name, captions = test_captions[i]  # Ambil nama gambar dan caption dari test set\n",
    "    img_path = os.path.join(images_path, img_name)  # Buat path lengkap gambar\n",
    "\n",
    "    # Hasilkan caption untuk gambar\n",
    "    generated_caption = generate_caption(img_path, model, tokenizer, feature_extractor)\n",
    "\n",
    "    # Simpan data gambar, caption asli, dan caption hasil prediksi ke dalam list\n",
    "    samples.append({\n",
    "        \"image\": img_name,          # Nama file gambar\n",
    "        \"ground_truth\": captions,   # Caption asli (daftar caption)\n",
    "        \"generated\": generated_caption  # Caption yang dihasilkan model\n",
    "    })\n",
    "\"\"\"\n",
    "Penjelasan:\n",
    "- Iterasi ini mengambil beberapa gambar dari test set dan menghasilkan caption untuk masing-masing.\n",
    "- Caption yang dihasilkan dibandingkan dengan caption asli.\n",
    "\"\"\"\n",
    "\n",
    "# Simpan sampel evaluasi ke file JSON\n",
    "with open(\"evaluation_samples.json\", \"w\") as f:\n",
    "    json.dump(samples, f, indent=4)\n",
    "\"\"\"\n",
    "Penjelasan:\n",
    "- `json.dump`: Menyimpan data `samples` ke dalam file JSON.\n",
    "- `indent=4`: Mengatur format file JSON agar lebih mudah dibaca.\n",
    "- \"evaluation_samples.json\": Nama file tempat data evaluasi disimpan.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Caption: a man playing guitar.\n"
     ]
    }
   ],
   "source": [
    "# Test the function\n",
    "test_image_path = \"example_pics/ello_gitar.jpg\"  # Replace with your image path\n",
    "print(\"Generated Caption:\", generate_caption(test_image_path, model, tokenizer, feature_extractor))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep-learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
